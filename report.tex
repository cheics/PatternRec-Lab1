\documentclass[article, 1.5space, letterpaper, 12pt, oneside, header, footer]{SydeClass}
\graphicspath{{images/}}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{eqnarray}
\usepackage{rotating}

\usepackage{array}
\usepackage{multirow}




% --------- Title Info -----------
\titlestyle{design} % used in SydeTitle.tex. Can equal one of the following values: design, work

\title{Lab 1}
\subtitle{Clusters and Classification Boundaries}

\coursecode{SYDE 472}
\department{Systems Design Engineering}

\author{Colin Heics, 20240543}
\authorheader{C. Heics}
\authortwo{Rob Sparrow, 20275155}
\authorheadertwo{R. Sparrow}
\authorthree{Philip Wang, 20276999}
\authorheaderthree{P. Wang}

\date{\today}
\instructor{Alex Wong}

\subsectionfont{\normalsize}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{1}

\input{matlabFormating}

% ############  ############
\begin{document}

% ---------- Title ------------
\input{SydeTitle}

% ############ Chapters ############
\pagenumbering{arabic}

\include{10_introduction}
\include{20_generatingClusters}
\include{30_classifiers}



\subsection{Results}
All classification methods were applied in each of Case 1 and Case 2. To aid in analysis, MED, GED, and MAP classifier boundaries are all plotted together in one figure, along with data clusters and unit standard deviation contours. In the Case 1 scenario (Figure~\ref{fig:med_ged_map_classifier_case1}), the GED and MAP classification boundaries, shown as magenta and blue lines respectively, lie on top of each other with the magenta line being obscured. This is because the MAP classifier is reduced to GED in this case, as the a priori probabilities for each class are equal. The classification boundaries are essentially two very slightly curved, but relatively straight, lines separating the two data clusters. The MED classification boundary is represented by the steeper straight cyan line. This makes sense intuitively, as the classification boundary represents the perpendicular bisector of the mean in each cluster. In the MED case only the mean, and not the covariance information for the two classes, is considered.

\begin{figure}[ht]
\centering
	{
	\includegraphics[width=0.45\linewidth]{fig2a-AB_MED_MICD_MAP}
	}
	
	\caption{MED, GED, and MAP classification boundaries for Case 1}
	\label{fig:med_ged_map_classifier_case1}
\end{figure}

In the Case 2 scenario (Figure~\ref{fig:med_ged_map_classifier_case2}), the MED classification boundary is shown as the light blue lines. The MED classification boundary is shown to be three straight lines, interescting near the mid-point between the three data clusters. The MED classifier does not take into account the covariance matrices of the three data clusters, so the performance described is as expected. The GED classification boundary is shown as the contoured magenta lines in the figure, intersecting near the midpoint between the three classes of data. This is an intuitive result, as the classification boundary better wraps around the unit standard deviation contours for three data classes. The MAP decision boundary is shown as the blue lines, slightly offset to the left of the GED decision boundary. In this case the decision boundaries do not overlay each other as in the Case 1 scenario. This is a result of the a priori probabilities for each of the three classes being different in this case, with the probability information altering the performance of the classification method.

\begin{figure}[ht]
\centering
	{
	\includegraphics[width=0.45\linewidth]{fig2b-AB_MED_MICD_MAP}
	}
	
	\caption{MED, GED, and MAP classification boundaries for Case 2}
	\label{fig:med_ged_map_classifier_case2}
\end{figure}

Next, the NN and 5NN classification boundaries were plotted together for both the Case 1 and Case 2 data sets, along with unit variance contours for each class, to allow for comparison. The decision boundaries for Case 1 (Figure~\ref{fig:nn_boundary_case1}) are fairly similar for both methods, with the decision boundaries being shown as jagged lines separating the data. The key difference in the two methods is that the 5NN classification method does not result in classification boundaries around outliers of the two data sets.

\begin{figure}[ht]
\centering
	\subfigure[NN classification for Case 1]{
	\includegraphics[width=0.45\linewidth]{fig3a-AB_NN}
	}
	\subfigure[5NN classification for Case 1]{
	\includegraphics[width=0.45\linewidth]{fig4a-AB_5NN}
	}
	
	\caption{Nearest Neighbor boundaries for Case 1}
	\label{fig:nn_boundary_case1}
\end{figure}

For the Case 2 scenario (Figure~\ref{fig:nn_boundary_case2}), performance of the two methods was similar. Again, the sensitivity of the NN method to outliers is seen to result to decision boundaries encircling outliers. The 5NN classifier is not as prone to these outliers, resulting in a more intuitive decision boundary between the three classes of data.

\begin{figure}[ht]
\centering
	\subfigure[NN classification for Case 2]{
	\includegraphics[width=0.45\linewidth]{fig3b-CDE_NN}
	}
	\subfigure[5NN classification for Case 2]{
	\includegraphics[width=0.45\linewidth]{fig4b-CDE_5NN}
	}
	
	\caption{Nearest Neighbor boundaries for Case 2}
	\label{fig:nn_boundary_case2}
\end{figure}

\include{50_errorAnal}

\section{Conclusions}
Implementation of five classifiers (MED, GED, MAP, NN, and 5NN) in MatLab was used to analyze the performance of each classifier. First, each method was implemented and used to classify a two-class and three-class data set. The classifiers were used to determine decision boundaries in each case, resulting in some initial observations about model performance. Overall, MED performed poorly, with GED and MAP classifiers performing significantly better. It was also seen that the 5NN classifier outperformed the NN classifier in rejecting statistical outliers in the data.

As noted in the earlier error analysis, MAP outperformed the MED, GED, NN, and 5NN classifiers in the scenario of Case 1. This is because MAP inherently attempts to minimize the probability of error, resulting in it outperforming other classifiers in terms of probability of error. MED does not consider the probability of error, or the the covariance of the data, and therefore performs relatively poorly. GED performs significantly better than MED for Case 1, and almost performs at the level of the MAP classifier. This is because GED considers the covariance of each class of data, though unlike MAP it does not consider the probability of error. The NN and 5NN classifiers are markedly outperformed by the other techniques, a result of them poorly classifying the new set of test data.

In the scenario of Case 2 the probability of error increased significantly for all classifiers. However, the same trend of MAP outperforming GED and MED, and GED outperforming MED, was conserved. It is notable in this case, however, that the performance of the NN and 5NN classifiers was competitive with other methods. The 5NN method still performed better than the NN method, due to it's ability to reject outliers. The improvement in performance of the 5NN and NN techniques here is likely due to both methods being able to account for the number of data points in each class and adjust accordingly - something the other classifiers are unable to accomplish.

\include{20_generatingClusters}

\include{99_matlabCodez}

% -------- Bibliography --------
%\addcontentsline{toc}{chapter}{\hspace{13pt} References}
\bibliography{refs}

\end{document}  